# 1.1 Pytorch 简介

## 1.1.1 PyTorch的由来
很多人都会拿PyTorch和Google的Tensorflow进行比较，这个肯定是没有问题的，因为他们是最火的两个深度学习框架了。但是说到PyTorch，其实应该先说[Torch](http://torch.ch)。

## 1.1.2 Torch是什么？

**Torch英译中：火炬**
- Torch是一个与Numpy类似的张量（Tensor）操作库，与Numpy不同的是Torch对GPU支持的很好，Lua是Torch的上层包装。
- PyTorch和Torch使用包含所有相同性能的C库：TH, THC, THNN, THCUNN，并且它们将继续共享这些库。
- 其实PyTorch和Torch都使用的是相同的底层，只是使用了不同的上层包装语言。
- 注：LUA虽然快，但是太小众了，所以才会有PyTorch的出现。

## 1.1.3 重新介绍 PyTorch
PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。 它主要由Facebook的人工智能研究小组开发。Uber的"Pyro"也是使用的这个库。

PyTorch是一个Python包，提供两个高级功能：
* 具有强大的GPU加速的张量计算（如NumPy）
* 包含自动求导系统的的深度神经网络

## 1.1.4 再次总结

- PyTorch算是相当简洁优雅且高效快速的框架
- 设计追求最少的封装，尽量避免重复造轮子
- 算是所有的框架中面向对象设计的最优雅的一个，设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法
- 大佬支持,与google的Tensorflow类似，FAIR的支持足以确保PyTorch获得持续的开发更新
- 不错的的文档（相比FB的其他项目，PyTorch的文档简直算是完善了，参考Thrift），PyTorch作者亲自维护的论坛 供用户交流和求教问题
- 入门简单

## 1.1.5 PyTorch
基于Python的科学计算包，服务于以下两种场景:
-  作为NumPy的替代品，可以使用GPU的强大计算能力
-  提供最大的灵活性和高速的深度学习研究平台

## 1.1.6 PyTorch的用途

**1.张量包含一个统一类型的数据**

**张量之间的张量计算依赖于类型和设备**

1. Tensors（张量）
- Tensors与Numpy中的 ndarrays类似
- Tensors 可以使用GPU进行计算.

>tensor([[0.0000, 0.0000, 0.0000],

>        [0.0000, 0.0000, 0.0000],

>        [0.0000, 0.0000, 0.0000],

>        [0.0000, 0.0000, 0.0000],

>        [0.0000, 0.0000, 0.0000]])

代码：x = torch.empty(5, 3)
        print(x)
 
>创建一个0填充的矩阵，数据类型为long:

x = torch.zeros(5, 3, dtype=torch.long)
print(x)
>创建tensor并使用现有数据初始化:

x = torch.tensor([5.5, 3])
print(x)
 
>根据现有的张量创建张量。 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖

x = x.new_ones(5, 3, dtype=torch.double)      # new_* 方法来创建对象\n",
print(x)
x = torch.randn_like(x, dtype=torch.float)    # 覆盖 dtype!\n",
print(x)                                      #  对象的size 是相同的，只是值和类型发生了变化"
  
## NumPy 转换
将一个Torch Tensor转换为NumPy数组是一件轻松的事，反之亦然。

Torch Tensor与NumPy数组共享底层内存地址，修改一个会导致另一个的变化。

将一个Torch Tensor转换为NumPy数组

a = torch.ones(5)

print(a)

>tensor([1., 1., 1., 1., 1.])

b = a.numpy()

print(b)

>观察numpy数组的值是如何改变的

>[1., 1., 1., 1., 1.]

a.add_(1)

print(a)

>tensor([2., 2., 2., 2., 2.])

print(b)

**所有的 Tensor 类型默认都是基于CPU， CharTensor 类型不支持到 NumPy 的转换. CUDA 张量**
print(t.device)

>cpu

print(t.layout)

>torch.strided

print(t.dtype)

>torch.float32

>将Tensor 类型转化为cuda

device =torch.device('cuda:0')
device

>device(type='cuda', index=0)

>整型和浮点转换

x=np.array([5,3])

torch.Tensor(x)

>tensor([5., 3.])#为float32

torch.tensor(x)//torch.as_tensor(x)//torch.from_numpy(x)

>tensor([5, 3])#为int32

torch.eye(3)

>tensor([[1., 0., 0.],

>        [0., 1., 0.],

>        [0., 0., 1.]])
>即单位张量

torch.zeros(3,3)

>tensor([[0., 0., 0.],

>        [0., 0., 0.],

>        [0., 0., 0.]])
>即零张量

torch.ones(3,3)

>tensor([[1., 1., 1.],

>        [1., 1., 1.],

>        [1., 1., 1.]])
>即一张量,注意为浮点

torch.rand(3,3)

>tensor([[0.6752, 0.7667, 0.6347],

>        [0.7315, 0.9881, 0.7442],

>        [0.7471, 0.8665, 0.8368]])
>即随机张量

>注意：torch.tensor(np.array([1,2,3]))与torch.tensor(np.array([1.,2.,3.]))有区别，前者为int32，后者为float64

-------

data=torch.array([1,2,3])

t1=torch.tensor(data)

t2=torch.Tensor(data)

t3=torch.as_tensor(data)

t4=torch.from_numpy(data)

print(t1)

>tensor([1, 2, 3])

print(t2)

>tensor([1., 2., 3.])

print(t3)

>tensor([1, 2, 3])

print(t4)

>tensor([1, 2, 3])

data[0]=0

data[1]=0

data[2]=0

print(t1)

>tensor([1, 2, 3])

print(t2)

>tensor([1., 2., 3.])

print(t3)

>tensor([0, 0, 0])

print(t4)

>tensor([0, 0, 0])

>原因：前两个生成数据副本，后两个共享数据

-------

## 张量的相关操作
1. 重塑操作
2. 元素操作
3. 还原操作
4. 访问操作

# 1.重塑操作

t=torch.tensor([[1,2,3],[4,5,6],[7,8,9]])

print(t)

>tensor([[1, 2, 3],

>        [4, 5, 6],

>        [7, 8, 9]])

print(t.size())

>torch.Size([3, 3])

t.shape()

>torch.Size([3, 3])

len(t.shape)

>2
>（指的是维度）

torch.tensor(t.shape).prod()
>求张量的标量分量

>tensor(9)

t.numel()
>(numel()返回张量中元素的数量)

>9

t.reshape([1,9])

>tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])

t.reshape([9,1])


>tensor([[1],

>        [2],

>        [3],

>        [4],

>        [5],

>        [6],

>        [7],

>        [8],

>        [9]])

------

t.reshape([1,9])

>tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])

t.reshape([1,9]).shape()

>torch.Size([1, 9])

t.reshape([1,9]).squeeze()

>tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])

t.reshape([1,9]).squeeze().shape()

>torch.Size([9])

>squeeze()函数用于删除张量形状中大小为1的维度

------

t.reshape([1,9]).squeeze().unsqueeze()

>tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])

t.reshape([1,9]).squeeze().unsqueeze(0)

>tensor([[[1, 2, 3, 4, 5, 6, 7, 8, 9]]])

>unsqueeze()函数用于在指定位置插入一个大小为1的维度

------










